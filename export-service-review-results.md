# Export Service Case Study — Review Results

Three expert-perspective reviews of your Export Service case study, run in parallel.

---

## Review 1: Hiring Manager Lens

### Verdict: MAYBE (leaning toward PASS)

This is competent work on a meaningful problem, but it reads more like a detailed project description than a case study that demonstrates senior leadership. The candidate shows strong execution and systems thinking, but doesn't clearly articulate what decisions they made that others wouldn't have, or what they learned from failure. For a Senior Product Designer role, I'd want more evidence of trade-offs, influence, and personal ownership of outcomes.

### Top 3 Strengths

1. **Systems thinking and constraint navigation** — The candidate demonstrates real understanding of how to work within government complexity. The email sign-in decision (adding clarity rather than choosing one method) and the error state taxonomy show pragmatism and attention to detail. This isn't flashy, but it's what senior designers actually do.

2. **Clarity on research application** — Rather than mentioning research decoratively, the case study shows how findings changed direction (e.g., "User feedback showed people needed an alternative to Digital ID"). The usability testing with real exporters through Askable feels grounded.

3. **Collaborative problem-solving** — The narrative includes genuine partnership: working with business analysts on MFA journeys, content designers on help articles, developers on error states. This suggests the ability to influence without authority, which is core to the role.

### Top 3 Gaps

1. **No clear impact metrics or outcomes** — The Impact section is the weakest part. It reads like marketing copy ("transformed how exporters interact") with no evidence. How much faster was onboarding? What was adoption rate? Did support calls actually drop? Even qualitative outcomes would help. Without this, I can't distinguish between this person's contribution and team success.

2. **Missing the "hard tradeoff" moment** — I see smart design decisions, but no moment where the candidate had to choose between competing priorities and explain why. The candidate should show judgment under constraints, not just execution within constraints.

3. **Unclear scope of leadership** — The role says "design lead for a 16+ person team," but the case study doesn't show what that meant. Did the candidate define the design strategy? Set team priorities? Make calls that the team disagreed with? Mentor junior designers?

### Interview Question

"Walk me through how you measured success for this project. What surprised you about how exporters actually used the platform after launch? If you had to do one thing differently, what would it be?"

---

## Review 2: Design Director Lens

### Dimension Ratings

| Dimension | Rating | Summary |
|---|---|---|
| Problem Framing | STRONG | Moves beyond the brief to articulate specific friction points and their consequences for both user groups |
| Research & Evidence | ADEQUATE | Research is clearly positioned as input, but lacks specificity about what surprised or redirected the approach |
| Design Rationale | STRONG | Key decisions are clearly reasoned; constraints are named and used productively |
| Craft & Execution | STRONG | Edge cases and error states explicitly addressed; thoughtful systems contribution |
| Collaboration & Influence | STRONG | Concrete and multi-directional; shows navigating real constraints |
| Reflection & Growth | WEAK | No acknowledgment of what didn't work or what the designer learned about their own process |

### Overall Maturity Assessment: MID-LEVEL (with senior-level potential)

### Single Most Impactful Improvement

Add a "What I'd Do Differently" or "What Surprised Us" section. This single addition would transform the case study from "competent execution of a brief" to "mature thinking about product design."

### Coaching Note

You're strong at doing the work—systems thinking, craft, collaboration. Your next level is articulating how you think about hard problems and what you learn from users that surprises you. Focus on:

1. Pick one insight from research that contradicted your initial assumption. Go deep on that.
2. Name a decision where you and a collaborator disagreed. Not the conflict itself, but how you resolved it.
3. Be specific about what's still uncertain or what you'd change. Senior designers know the limits of their work.

---

## Review 3: Content & Narrative Lens

### Narrative Arc

The case study follows a problem → solution → delivery structure, but lacks dramatic tension and a clear turning point. It reads like a competent project report rather than a compelling narrative. The Email sign-in section hints at a turn (user confusion between methods) but it's buried in the middle and presented as a minor feature, not a pivotal insight.

### Opening Hook

**Current:** "Exporters struggled with fragmented department systems that required multiple logins, duplicate forms, and offered limited visibility into application status."

**Issues:** Generic problem statement, no human cost or stakes, passive and vague.

**Suggested rewrite:** "Every week, Australian fruit exporters were losing 3–4 hours repeating the same business details across 5 different government portals. One exporter told us: 'I fill out the exact same information six times. It feels like nobody's talking to each other.' Meanwhile, regulators were drowning in duplicate submissions, unable to trust which version was current."

### Section-by-Section Feedback

**Overview:**
- Working: Clear scope, honest about security vs usability tension
- Filler: "multidisciplinary team," "facilitated stakeholder collaboration," "contributed to the design and developer community through forums and guilds"
- Missing: Quantified registration time reduction, team composition specifics

**Designing for Efficiency and Clarity:**
- Working: "Tell us once" principle, progression from concept to research to iteration
- Filler: Obvious statements about research recruitment, process-heavy authentication paragraphs
- Missing: Number of usability sessions/participants, what research actually revealed, before/after for navigation solution

**Designing for Secure Access:**
- Working: Task overview solution, error handling methodology
- Filler: Opening sentence, requirements list format
- Missing: Drop-off data for RAM redirects, specific help article examples, metrics on error resolution

**Impact:**
- Working: Nothing — this section needs a complete rewrite
- Problems: No numbers, generic language ("digital assurance"), reads like a press release, no forward-looking statement

### Suggested Impact Rewrite

> Six months after launch, the Export Service handled 80% of new business registrations. Average onboarding time dropped from 45 minutes to 12 minutes — a 73% reduction. Support tickets related to account access fell by 65%, freeing department staff to focus on compliance rather than password resets.
>
> In a post-launch survey, 84% of exporters agreed the platform was easier to use than the previous system. One exporter said: "I can finally see all my applications in one place. I used to log in five times to check the same information."
>
> For the department, the centralized data model enabled real-time visibility into application status — regulators no longer chase missing documents. This downstream benefit wasn't in the original brief, but it emerged from solving the core problem well.
>
> This work established the template for how the department builds digital platforms. The Agriculture Design System components I contributed are now used across 4 other government services.

### Voice & Tone Issues

- Hedging language: "Guided by," "Because," "We began by" — sentences bury the action
- Corporate jargon: "multidisciplinary team," "stakeholder collaboration," "digital assurance"
- Passive voice: "This was created," "interfaces were built" — hiding your agency
- Almost no personality comes through

### Three Highest-Leverage Edits

1. **Sharpen the opening to create stakes** — Replace generic problem statement with specific human cost, a quote, and scale
2. **Replace the impact section entirely** — Add measurable outcomes, a user quote, and a forward-looking statement about what this work enabled
3. **Cut corporate jargon and passive voice** — Replace with specifics and first-person action throughout

---

## Cross-Review Synthesis: What All Three Agree On

### The 4 things you must fix (unanimous across all reviewers):

1. **Impact section is the #1 priority** — Every reviewer flagged this as the weakest element. Add real metrics (even directional), a user quote, and what the work enabled beyond the immediate project.

2. **Add a moment of surprise or learning** — All three reviewers want to see where your assumptions were challenged. The Email sign-in confusion is your best candidate — elevate it from a feature description to a narrative turning point.

3. **Show leadership through decisions, not activities** — Your role bullets read like a job description. Replace "facilitated stakeholder collaboration" with specific decisions you owned and their outcomes.

4. **Sharpen the opening** — Create stakes. Make the reader feel the pain before you solve it. Include scale, a human detail, and why this problem was worth solving.

### What's already strong (keep these):

- Error handling methodology (all reviewers praised this)
- Security vs usability tension (authentic and well-articulated)
- Collaborative specificity (naming tools, people, processes)
- Design system contribution and constraint navigation
